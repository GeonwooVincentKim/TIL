{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 | CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rZxjVBjtqpVMqk95JQ3psbWbBORmHcOZ",
      "authorship_tag": "ABX9TyOmZoMmiIbihsLE0HjSe6B5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyelinNAM/TIL/blob/master/GAN/0407_CycleGAN\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1dWTCEvn3j",
        "colab_type": "text"
      },
      "source": [
        "## Image-to-Image Translation (Photo ↔ VanGogh) using CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meLpTv_Rh7Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6FZCLhW-axg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "36945dca-d4a3-4112-9dca-509ebac16b32"
      },
      "source": [
        "# Set random seed for reproductibility \n",
        "manualSeed = 999\n",
        "print(\"Random seed:\",manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed: 999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f528d601a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJeCVmQPv2K1",
        "colab_type": "code",
        "outputId": "8ec3252e-dde6-402b-fd0d-e1bb3d4f1f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXjr9bQyWGpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A > 반고흐, B > 사진\n",
        "\n",
        "A_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/vangogh2photo/trainA/'\n",
        "B_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/vangogh2photo/trainB/'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))           \n",
        "])\n",
        "\n",
        "A_data = dset.ImageFolder(root=A_root,transform=transform,)\n",
        "B_data = dset.ImageFolder(root=B_root,transform=transform,)\n",
        "\n",
        "A_dataloader = torch.utils.data.DataLoader(A_data,batch_size = 8,shuffle=True) # batch - 25개\n",
        "B_dataloader = torch.utils.data.DataLoader(B_data,batch_size = 8,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkZmYJFg1CsJ",
        "colab_type": "code",
        "outputId": "8cfb6840-c44c-4445-aa7a-dbde07d32330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(len(A_dataloader))\n",
        "print(len(B_dataloader))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapMt_tgiXRs",
        "colab_type": "code",
        "outputId": "2de86c39-34b6-48cf-84f6-3a99270d7b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "# For test\n",
        "'''tA_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/v2p_test/testA'\n",
        "tB_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/v2p_test/testB'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))           \n",
        "])\n",
        "\n",
        "tA_data = dset.ImageFolder(root=tA_root,transform=transform,)\n",
        "tB_data = dset.ImageFolder(root=tB_root,transform=transform,)'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"tA_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/v2p_test/testA'\\ntB_root='/content/drive/My Drive/Adv_GAN; Experiments/Model/CycleGAN/v2p_test/testB'\\n\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))           \\n])\\n\\ntA_data = dset.ImageFolder(root=tA_root,transform=transform,)\\ntB_data = dset.ImageFolder(root=tB_root,transform=transform,)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VezLpcTZCRsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_features):\n",
        "    super(ResidualBlock,self).__init__()\n",
        "\n",
        "    conv_block = [ nn.ReflectionPad2d(1),\n",
        "                   nn.Conv2d(in_features,in_features,3),\n",
        "                   nn.InstanceNorm2d(in_features),\n",
        "                   nn.ReLU(inplace=True),\n",
        "                   nn.ReflectionPad2d(1),\n",
        "                   nn.Conv2d(in_features,in_features,3),\n",
        "                   nn.InstanceNorm2d(in_features) ]\n",
        "\n",
        "    self.conv_block = nn.Sequential(*conv_block) # * - unpacking, Q. type?\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return x + self.conv_block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgPBKDvXQr3-",
        "colab_type": "text"
      },
      "source": [
        "*   feature map수 만 다르고 반복되는 구조의 모델이 많다 > 일부는 반복문으로 처리!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UNbz3cE1K1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator,self).__init__()\n",
        "\n",
        "    ### 1. Initial Convolution blocks (Encoder) ###\n",
        "\n",
        "    out_features = 64\n",
        "\n",
        "    # input > 128\n",
        "    model = [ nn.ReflectionPad2d(1), # 130\n",
        "              nn.Conv2d(3,64,3,1), # (130-3)/1 + 1 = 128\n",
        "              nn.InstanceNorm2d(64),\n",
        "              nn.ReLU(inplace=True) ]\n",
        "\n",
        "    ## Downsampling ## > 피처맵 사이즈\n",
        "    \n",
        "    in_features = out_features # 64\n",
        "    out_features = in_features * 2 # 128\n",
        "    \n",
        "    for i in range(2):\n",
        "      model += [ nn.Conv2d(in_features,out_features,3,2,1), # (128+2-3)/2 = \n",
        "                 nn.InstanceNorm2d(out_features),\n",
        "                 nn.ReLU(inplace=True) ]\n",
        "      in_features = out_features \n",
        "      out_features = in_features * 2\n",
        "      # in/out ) 64/128, 128/256\n",
        "      # 반복문 빠져나온 뒤) in = 256 out= 512\n",
        "    \n",
        "    ### 2. Residual blocks ### 논문 > 9개, 6개로 줄임.\n",
        "    for i in range(6):\n",
        "      model += [ResidualBlock(in_features)]\n",
        "      # 패딩으로 조절해, 피처맵 사이즈 유지 > \n",
        "\n",
        "    ### 3. Convolution blocks (Decoder)  ###\n",
        "\n",
        "    # in = 256, out = 512\n",
        "\n",
        "    ## Upsampling ##\n",
        "\n",
        "    out_features = in_features//2 # // - 정수형 나눗셈\n",
        "\n",
        "    for i in range(2):\n",
        "      model += [ nn.ConvTranspose2d(in_features,out_features,3,2,1,output_padding=1),\n",
        "                 nn.InstanceNorm2d(out_features),\n",
        "                 nn.ReLU(inplace=True) ]\n",
        "      \n",
        "      in_features = out_features\n",
        "      out_features = in_features//2\n",
        "\n",
        "    ## Output layer ##\n",
        "    model += [ nn.ReflectionPad2d(3),\n",
        "               nn.Conv2d(64,3,7),\n",
        "               nn.Tanh()]\n",
        "\n",
        "    self.model = nn.Sequential(*model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMT4Zbt9Zsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "\n",
        "    model = [ nn.Conv2d(3,64,4,stride=2,padding=1),\n",
        "              nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "    model += [ nn.Conv2d(64,128,4,2,1),\n",
        "               nn.InstanceNorm2d(128),\n",
        "               nn.LeakyReLU(0.2,True) ]\n",
        "\n",
        "    model += [ nn.Conv2d(128,256,4,2,1),\n",
        "               nn.InstanceNorm2d(256),\n",
        "               nn.LeakyReLU(0.2,True) ]\n",
        "\n",
        "    # FCN classification layer\n",
        "\n",
        "    model += [ nn.Conv2d(256,1,4,1)]\n",
        "\n",
        "    self.model = nn.Sequential(*model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.model(x) # [8,1,10,10]\n",
        "    \n",
        "    # Average pooling and flatten\n",
        "    x = F.avg_pool2d(x,x.size()[2:]).view(x.size()[0],-1) # [8,1]\n",
        "    x = torch.sigmoid(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8xoY05VG3QM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "\n",
        "  if classname.find('Conv') != -1:\n",
        "    m.weight.data.normal_(0.0,0.02)\n",
        "  \n",
        "  '''elif classname.find('Norm') != -1:\n",
        "    m.weight.data.normal_(1.0,0.02)'''\n",
        "    # Q. InstanceNorm weight?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PIVlyz0CY92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "44f13c17-7c32-4d7c-fc42-528d89f8f6ca"
      },
      "source": [
        "### Definition of variables ###\n",
        "\n",
        "import itertools\n",
        "\n",
        "# A > 반고흐, B > 사진\n",
        "G_A2B = Generator().to(device)\n",
        "G_B2A = Generator().to(device)\n",
        "D_A = Discriminator().to(device)\n",
        "D_B = Discriminator().to(device)\n",
        "\n",
        "G_A2B.apply(weights_init)\n",
        "G_B2A.apply(weights_init)\n",
        "D_A.apply(weights_init)\n",
        "D_B.apply(weights_init)\n",
        "\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_Cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "lr = 0.0002 # default learning rate\n",
        "\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(),G_B2A.parameters()),lr=lr,betas=(0.5,0.999))\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(),lr=lr,betas=(0.5,0.999))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(),lr=lr,betas=(0.5,0.999))\n",
        "\n",
        "'''class LambdaLR():\n",
        "  def __init__(self,epochs,offset,decay_epoch):\n",
        "    assert ((epochs - decay_epoch)> 0), \"학습 종료 전 decay를 시작해야합니다.\"\n",
        "    self.epochs = epochs\n",
        "    self.offset = offset\n",
        "    self.decay_epoch = decay_epoch\n",
        "    \n",
        "  def step(self,epoch):\n",
        "    return 1.0 - max(0,epochs+self.offset-self.decay_epoch)/(self.epochs-self.decay_epoch)\n",
        "\n",
        "epochs = 200\n",
        "offset = 0\n",
        "decay_epoch = 100\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)'''\n",
        "\n",
        "# Lambda 함수 지정해 그에 따라 learning rate 변화 줌 > StepLR()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class LambdaLR():\\n  def __init__(self,epochs,offset,decay_epoch):\\n    assert ((epochs - decay_epoch)> 0), \"학습 종료 전 decay를 시작해야합니다.\"\\n    self.epochs = epochs\\n    self.offset = offset\\n    self.decay_epoch = decay_epoch\\n    \\n  def step(self,epoch):\\n    return 1.0 - max(0,epochs+self.offset-self.decay_epoch)/(self.epochs-self.decay_epoch)\\n\\nepochs = 200\\noffset = 0\\ndecay_epoch = 100\\n\\nlr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)\\nlr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)\\nlr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B,lr_lambda = LambdaLR(epochs,offset,decay_epoch).step)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO3iRcaWcsKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def img_show(data1,data2,epoch):\n",
        "  f = plt.figure(figsize=(10,5))\n",
        "  plt.title(f'{epoch+1} - image')\n",
        "\n",
        "  A2B = f.add_subplot(2,1,1)\n",
        "  B2A = f.add_subplot(2,1,2)\n",
        "\n",
        "  A2B.axis(\"off\")\n",
        "  B2A.axis(\"off\")\n",
        "  \n",
        "  A2B.imshow(np.transpose(vutils.make_grid(data1.to(device)[:5], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "  B2A.imshow(np.transpose(vutils.make_grid(data2.to(device)[:5], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "  if (epoch+1) < 10:\n",
        "    f.savefig(f'0{epoch+1}')\n",
        "\n",
        "  else:\n",
        "    f.savefig(f'{epoch+1}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2qM8xq-4gcjQ",
        "colab": {}
      },
      "source": [
        "'''for A_real, B_real in zip(A_dataloader,B_dataloader):\n",
        "  image, label = A_real\n",
        "  print(len(image))\n",
        "  print(image.shape)\n",
        "  break'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IGIjOzAappb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Training ###\n",
        "\n",
        "epochs = 100\n",
        "#decay_epoch = 100\n",
        "\n",
        "print('Start Training Loop...')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  iter_ = 1\n",
        "\n",
        "  epoch_start_time = time.time()\n",
        "\n",
        "  for A_real, B_real in zip(A_dataloader,B_dataloader): # 작은 개수에 맞춰 반복문 > 25개\n",
        "\n",
        "    A_real = A_real[0].to(device) \n",
        "    B_real = B_real[0].to(device) # torch.Size([16, 3, 256, 256])\n",
        "\n",
        "    batch_size = A_real.size()[0]\n",
        "    target_real = torch.ones(batch_size).to(device)\n",
        "    target_fake = torch.zeros(batch_size).to(device)\n",
        "\n",
        "    ### 1.Loss for Generator ###\n",
        "\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    ## 1-1.Identity Loss ##\n",
        "    A_idt = G_B2A(A_real)\n",
        "    B_idt = G_A2B(B_real)\n",
        "\n",
        "    loss_idt_A = criterion_identity(A_idt,A_real)*5.0\n",
        "    loss_idt_B = criterion_identity(B_idt,B_real)*5.0\n",
        "\n",
        "    ## 1-2.GAN Loss ##\n",
        "    A_fake = G_B2A(B_real)\n",
        "    pred_fake = D_A(A_fake).squeeze() \n",
        "    loss_gan_B2A = criterion_GAN(pred_fake,target_real)\n",
        "  \n",
        "    B_fake = G_A2B(A_real)\n",
        "    pred_fake = D_B(B_fake).squeeze() \n",
        "    loss_gan_A2B = criterion_GAN(pred_fake,target_real)\n",
        "    \n",
        "    ## 1-3.Cycle Loss ##\n",
        "    A_recon = G_B2A(B_fake)\n",
        "    loss_cycle_ABA = criterion_Cycle(A_recon,A_real)*10.0\n",
        "\n",
        "    B_recon = G_A2B(A_fake)\n",
        "    loss_cycle_B2B = criterion_Cycle(B_recon,B_real)*10.0\n",
        "\n",
        "    ## 1-4.Total Loss ##\n",
        "    loss_G = loss_idt_A+loss_idt_B+loss_gan_A2B+loss_gan_B2A+loss_cycle_ABA+loss_cycle_B2B\n",
        "    loss_G_ = loss_G.mean().item()\n",
        "    loss_G.backward()\n",
        "\n",
        "    optimizer_G.step()\n",
        "\n",
        "    ### 2. Loss for Discriminator D_A ###\n",
        "\n",
        "    optimizer_D_A.zero_grad()\n",
        "\n",
        "    ## 2-1.using Real image ##\n",
        "    pred_real = D_A(A_real).squeeze() \n",
        "    loss_D_real = criterion_GAN(pred_real,target_real)\n",
        "    Prob_D_A_Real = pred_real.mean().item()\n",
        "\n",
        "    ## 2-2.using Fake image ##\n",
        "    pred_fake = D_A(A_fake.detach()).squeeze() \n",
        "    loss_D_fake = criterion_GAN(pred_fake,target_fake)\n",
        "    Prob_D_A_Fake = pred_fake.mean().item()\n",
        "\n",
        "    loss_D_A = (loss_D_real+loss_D_fake)*0.5\n",
        "    loss_D_A_ = loss_D_A.mean().item()\n",
        "    loss_D_A.backward()\n",
        "\n",
        "    optimizer_D_A.step()\n",
        "\n",
        "    ### 3. Loss for Discriminator D_B ###\n",
        "\n",
        "    optimizer_D_B.zero_grad()\n",
        "\n",
        "    ## 3-1.using Real image ##\n",
        "    pred_real = D_B(B_real).squeeze() \n",
        "    pred_real_ = pred_real.mean().item()\n",
        "    loss_D_real = criterion_GAN(pred_real,target_real)\n",
        "\n",
        "    ## 3-2.using Fake image ##\n",
        "    pred_fake = D_B(B_fake.detach()).squeeze() \n",
        "    pred_fake_ = pred_fake.mean().item()\n",
        "    loss_D_fake = criterion_GAN(pred_fake,target_fake)\n",
        "\n",
        "    loss_D_B = (loss_D_real+loss_D_fake)*0.5\n",
        "    loss_D_B_ = loss_D_B.mean().item()\n",
        "    loss_D_B.backward()\n",
        "\n",
        "    optimizer_D_B.step()\n",
        "\n",
        "    if iter_ % 5 == 0:\n",
        "      print(f'[{epoch+1}/{epochs}][{iter_}/{len(A_dataloader)}]\\tLoss_G:{loss_G_:.3f}\\tLoss_D_A:{loss_D_A_:.3f}\\tLoss_D_B:{loss_D_B_:.3f}\\\n",
        "      \\tProb_D_A(Real):{Prob_D_A_Real:.3f}\\tProb_D_A(Fake):{Prob_D_A_Fake:.3f}\\tProb_D_B(Real):{pred_real_:.3f}\\tProb_D_B(Fake):{pred_fake_:.3f}')\n",
        "\n",
        "    iter_ +=1\n",
        "\n",
        "  epoch_end_time = time.time()\n",
        "  per_epoch_time = epoch_end_time - epoch_start_time\n",
        "  print(f'[{epoch+1}/{epochs}] -ptime: {per_epoch_time:.2f} ')\n",
        "\n",
        "  '''lr_scheduler_G.step()\n",
        "  lr_scheduler_D_A.step()\n",
        "  lr_scheduler_D_B.step()'''\n",
        "\n",
        "  img_show(A_fake.detach(),B_fake.detach(),epoch)\n",
        " \n",
        "torch.save({\n",
        "    'G_A2B':G_A2B.state_dict(),\n",
        "    'G_B2A':G_B2A.state_dict(),\n",
        "    'D_A':D_A.state_dict(),\n",
        "    'D_B':D_B.state_dict(),\n",
        "    'Op_G':optimizer_G.state_dict(),\n",
        "    'Op_D_A':optimizer_D_A.state_dict(),\n",
        "    'Op_D_B':optimizer_D_B.state_dict(),\n",
        "    },'/content/drive/My Drive/VanillaCycleGAN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX_x2ut6kNpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "    'G_A2B':G_A2B.state_dict(),\n",
        "    'G_B2A':G_B2A.state_dict(),\n",
        "    'D_A':D_A.state_dict(),\n",
        "    'D_B':D_B.state_dict(),\n",
        "    'Op_G':optimizer_G.state_dict(),\n",
        "    'Op_D_A':optimizer_D_A.state_dict(),\n",
        "    'Op_D_B':optimizer_D_B.state_dict(),\n",
        "    },'/content/drive/My Drive/VanillaCycleGAN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQRHGmebuuWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}